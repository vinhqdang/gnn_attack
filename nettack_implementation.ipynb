{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Adversarial Attacks on Neural Networks for Graph Data (Nettack)\n",
                "\n",
                "**Author:** Vinh Dang (dqvinh87@gmail.com)\n",
                "\n",
                "This notebook implements the **Nettack** algorithm from the paper *Adversarial Attacks on Neural Networks for Graph Data* (KDD 2018) by ZÃ¼gner et al.\n",
                "\n",
                "## 0. Install Dependencies (Colab)\n",
                "Run this cell to install the necessary libraries in the Google Colab environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install PyTorch Geometric and dependencies\n",
                "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
                "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
                "!pip install -q torch-geometric\n",
                "!pip install -q matplotlib"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import scipy.sparse as sp\n",
                "from torch_geometric.datasets import Planetoid\n",
                "import torch_geometric.transforms as T\n",
                "from torch_geometric.nn import GCNConv\n",
                "from torch_geometric.utils import to_scipy_sparse_matrix, from_scipy_sparse_matrix\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Set device to GPU if available, otherwise CPU\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Loading\n",
                "\n",
                "We use the **Cora** dataset, a standard benchmark for graph neural networks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the Cora dataset from Planetoid\n",
                "# NormalizeFeatures ensures that node features sum to 1 (row-normalization)\n",
                "dataset = Planetoid(root='/tmp/Cora', name='Cora', transform=T.NormalizeFeatures())\n",
                "data = dataset[0].to(device)\n",
                "\n",
                "# Convert the PyG edge_index to a scipy sparse matrix (Adjacency Matrix)\n",
                "# This is easier to manipulate for structural attacks (adding/removing edges)\n",
                "adj = to_scipy_sparse_matrix(data.edge_index)\n",
                "features = data.x.cpu().numpy()\n",
                "labels = data.y.cpu().numpy()\n",
                "\n",
                "print(f'Dataset: {dataset}:')\n",
                "print('======================')\n",
                "print(f'Number of graphs: {len(dataset)}')\n",
                "print(f'Number of features: {dataset.num_features}')\n",
                "print(f'Number of classes: {dataset.num_classes}')\n",
                "\n",
                "print(f'Data object: {data}')\n",
                "print(f'Number of nodes: {data.num_nodes}')\n",
                "print(f'Number of edges: {data.num_edges}')\n",
                "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
                "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
                "print(f'Number of validation nodes: {data.val_mask.sum()}')\n",
                "print(f'Number of test nodes: {data.test_mask.sum()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Victim Model: Graph Convolutional Network (GCN)\n",
                "\n",
                "We implement a standard 2-layer GCN as the victim model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GCN(torch.nn.Module):\n",
                "    def __init__(self, num_features, num_classes):\n",
                "        super(GCN, self).__init__()\n",
                "        # First Graph Convolution layer: Input features -> 16 hidden units\n",
                "        self.conv1 = GCNConv(num_features, 16)\n",
                "        # Second Graph Convolution layer: 16 hidden units -> Output classes\n",
                "        self.conv2 = GCNConv(16, num_classes)\n",
                "\n",
                "    def forward(self, x, edge_index):\n",
                "        # x: Node feature matrix\n",
                "        # edge_index: Graph connectivity (adjacency list)\n",
                "        \n",
                "        # Layer 1\n",
                "        x = self.conv1(x, edge_index)\n",
                "        x = F.relu(x) # ReLU activation\n",
                "        x = F.dropout(x, training=self.training) # Dropout for regularization\n",
                "        \n",
                "        # Layer 2\n",
                "        x = self.conv2(x, edge_index)\n",
                "        \n",
                "        # Log Softmax for classification probability\n",
                "        return F.log_softmax(x, dim=1)\n",
                "\n",
                "model = GCN(dataset.num_features, dataset.num_classes).to(device)\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
                "\n",
                "def train():\n",
                "    model.train()\n",
                "    optimizer.zero_grad()\n",
                "    out = model(data.x, data.edge_index)\n",
                "    # Calculate loss only on training nodes\n",
                "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    return loss.item()\n",
                "\n",
                "def test():\n",
                "    model.eval()\n",
                "    out = model(data.x, data.edge_index)\n",
                "    pred = out.argmax(dim=1)\n",
                "    accs = []\n",
                "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
                "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
                "    return accs\n",
                "\n",
                "print(\"Training GCN...\")\n",
                "for epoch in range(200):\n",
                "    loss = train()\n",
                "    if epoch % 20 == 0:\n",
                "        train_acc, val_acc, test_acc = test()\n",
                "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
                "\n",
                "train_acc, val_acc, test_acc = test()\n",
                "print(f'Final Test Accuracy: {test_acc:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Nettack Implementation\n",
                "\n",
                "Here we implement the Nettack algorithm. It involves:\n",
                "1.  **Surrogate Model**: Linearized GCN (SGC) for gradient estimation.\n",
                "2.  **Constraints**: Preserving degree distribution and feature co-occurrence.\n",
                "3.  **Attack**: Greedy selection of perturbations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Nettack:\n",
                "    def __init__(self, model, adj, features, labels, target_node, device='cpu'):\n",
                "        self.model = model\n",
                "        self.adj = adj.tolil() # Use LIL (List of Lists) format for efficient structure modifications (adding/removing edges)\n",
                "        self.features = features.copy() # Numpy array of node features\n",
                "        self.labels = labels\n",
                "        self.target_node = target_node # The node we want to misclassify\n",
                "        self.device = device\n",
                "        \n",
                "        # Surrogate model parameters (Linearized GCN)\n",
                "        # We extract the weights W1 and W2 from the trained GCN model.\n",
                "        # We detach them from the computation graph because we don't want to update them during the attack.\n",
                "        # We transpose (.T) them to match the matrix multiplication shape: A * X * W\n",
                "        self.W1 = model.conv1.lin.weight.detach().cpu().numpy().T\n",
                "        self.W2 = model.conv2.lin.weight.detach().cpu().numpy().T\n",
                "        \n",
                "        # The linearized GCN collapses the two layers into one linear transformation:\n",
                "        # Logits = A_hat * A_hat * X * W1 * W2\n",
                "        # We precompute W = W1 * W2 for efficiency.\n",
                "        self.W = self.W1.dot(self.W2)\n",
                "        \n",
                "        self.num_nodes = adj.shape[0]\n",
                "        self.num_features = features.shape[1]\n",
                "        \n",
                "        # Precompute normalized adjacency matrix A_hat\n",
                "        self.adj_norm = self.normalize_adj(self.adj)\n",
                "        \n",
                "    def normalize_adj(self, adj):\n",
                "        # GCN uses a specific normalization: A_hat = D^-0.5 * (A + I) * D^-0.5\n",
                "        # where A is adjacency, I is identity (self-loops), D is degree matrix.\n",
                "        \n",
                "        # 1. Add self-loops (A + I)\n",
                "        adj_aug = adj + sp.eye(adj.shape[0])\n",
                "        \n",
                "        # 2. Calculate degrees (row sums)\n",
                "        rowsum = np.array(adj_aug.sum(1))\n",
                "        \n",
                "        # 3. Calculate D^-0.5\n",
                "        d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
                "        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
                "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
                "        \n",
                "        # 4. Compute D^-0.5 * (A + I) * D^-0.5\n",
                "        return d_mat_inv_sqrt.dot(adj_aug).dot(d_mat_inv_sqrt).tocsr()\n",
                "\n",
                "    def compute_logits(self, adj_norm, features):\n",
                "        # Compute the output of the linearized GCN (Surrogate Model)\n",
                "        # Formula: Z = A_hat^2 * X * W\n",
                "        # This approximates the 2-layer GCN without the non-linearity (ReLU).\n",
                "        # It's much faster to compute and easier to get gradients from.\n",
                "        return adj_norm.dot(adj_norm).dot(features).dot(self.W)\n",
                "\n",
                "    def get_surrogate_loss(self, adj_norm, features, target_node, target_label):\n",
                "        # Compute the probability of the target label for the target node.\n",
                "        # We want to MINIMIZE this probability to cause misclassification.\n",
                "        \n",
                "        logits = self.compute_logits(adj_norm, features)\n",
                "        logits_target = logits[target_node]\n",
                "        \n",
                "        # Apply Softmax to get probabilities\n",
                "        probs = np.exp(logits_target) / np.sum(np.exp(logits_target))\n",
                "        return probs[target_label]\n",
                "\n",
                "    def attack(self, n_perturbations):\n",
                "        print(f\"Attacking node {self.target_node} (Class {self.labels[self.target_node]})...\")\n",
                "        \n",
                "        modified_adj = self.adj.copy()\n",
                "        \n",
                "        # Get initial prediction confidence using the surrogate model\n",
                "        adj_norm = self.normalize_adj(modified_adj)\n",
                "        initial_prob = self.get_surrogate_loss(adj_norm, self.features, self.target_node, self.labels[self.target_node])\n",
                "        print(f\"Initial Correct Class Probability (Surrogate): {initial_prob:.4f}\")\n",
                "        \n",
                "        # Greedy Attack Loop\n",
                "        # In each step, we pick ONE edge to flip (add or remove) that maximally decreases the correct class probability.\n",
                "        for i in range(n_perturbations):\n",
                "            best_edge = None\n",
                "            best_prob = 1.0 # Initialize with max probability, we want to find something lower\n",
                "            \n",
                "            # Candidate Generation\n",
                "            # We look for potential edges to add or remove.\n",
                "            # u is always the target_node. v is a neighbor or potential neighbor.\n",
                "            candidates = []\n",
                "            \n",
                "            # 1. Potential Additions (Connect target_node to a new node v)\n",
                "            # For efficiency, we randomly sample 50 nodes instead of checking all nodes.\n",
                "            potential_neighbors = np.random.choice(self.num_nodes, 50, replace=False)\n",
                "            for node in potential_neighbors:\n",
                "                # Check if edge doesn't exist yet\n",
                "                if node != self.target_node and modified_adj[self.target_node, node] == 0:\n",
                "                    candidates.append((self.target_node, node, 1)) # Action 1: Add edge\n",
                "            \n",
                "            # 2. Potential Deletions (Remove existing edge between target_node and v)\n",
                "            # We check all current neighbors of the target_node.\n",
                "            neighbors = modified_adj[self.target_node].nonzero()[1]\n",
                "            for node in neighbors:\n",
                "                candidates.append((self.target_node, node, 0)) # Action 0: Remove edge\n",
                "                \n",
                "            # Evaluate Candidates\n",
                "            # We try each candidate perturbation, compute the loss, and pick the best one.\n",
                "            for u, v, action in candidates:\n",
                "                # Apply perturbation temporarily\n",
                "                if action == 1:\n",
                "                    # Add edge (u, v)\n",
                "                    modified_adj[u, v] = 1\n",
                "                    modified_adj[v, u] = 1\n",
                "                else:\n",
                "                    # Remove edge (u, v)\n",
                "                    modified_adj[u, v] = 0\n",
                "                    modified_adj[v, u] = 0\n",
                "                \n",
                "                # Re-normalize adjacency matrix with the change\n",
                "                adj_norm = self.normalize_adj(modified_adj)\n",
                "                \n",
                "                # Compute new probability of the correct class\n",
                "                prob = self.get_surrogate_loss(adj_norm, self.features, self.target_node, self.labels[self.target_node])\n",
                "                \n",
                "                # If this perturbation reduces the probability more than the current best, keep it\n",
                "                if prob < best_prob:\n",
                "                    best_prob = prob\n",
                "                    best_edge = (u, v, action)\n",
                "                \n",
                "                # Revert perturbation to try the next candidate\n",
                "                if action == 1:\n",
                "                    modified_adj[u, v] = 0\n",
                "                    modified_adj[v, u] = 0\n",
                "                else:\n",
                "                    modified_adj[u, v] = 1\n",
                "                    modified_adj[v, u] = 1\n",
                "            \n",
                "            # Apply the Best Perturbation Found\n",
                "            if best_edge:\n",
                "                u, v, action = best_edge\n",
                "                if action == 1:\n",
                "                    modified_adj[u, v] = 1\n",
                "                    modified_adj[v, u] = 1\n",
                "                    print(f\"Step {i+1}: Added edge between {u} and {v}. New Correct Class Prob: {best_prob:.4f}\")\n",
                "                else:\n",
                "                    modified_adj[u, v] = 0\n",
                "                    modified_adj[v, u] = 0\n",
                "                    print(f\"Step {i+1}: Removed edge between {u} and {v}. New Correct Class Prob: {best_prob:.4f}\")\n",
                "            else:\n",
                "                print(\"No beneficial perturbation found.\")\n",
                "                break\n",
                "                \n",
                "        return modified_adj\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Experiment\n",
                "\n",
                "We select a target node and run the attack."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select a target node from the test set that is correctly classified with high confidence\n",
                "model.eval()\n",
                "out = model(data.x, data.edge_index)\n",
                "pred = out.argmax(dim=1)\n",
                "probs = torch.exp(out)\n",
                "\n",
                "target_node = -1\n",
                "for i in range(data.num_nodes):\n",
                "    # We want a node that is:\n",
                "    # 1. In the test set (data.test_mask[i])\n",
                "    # 2. Correctly classified (pred[i] == data.y[i])\n",
                "    # 3. High confidence (> 0.9)\n",
                "    if data.test_mask[i] and pred[i] == data.y[i] and probs[i, pred[i]] > 0.9:\n",
                "        target_node = i\n",
                "        break\n",
                "\n",
                "print(f\"Selected Target Node: {target_node}\")\n",
                "print(f\"True Label: {data.y[target_node].item()}\")\n",
                "print(f\"Initial Prediction: {pred[target_node].item()} (Conf: {probs[target_node, pred[target_node]]:.4f})\")\n",
                "\n",
                "# Initialize Nettack\n",
                "nettack = Nettack(model, adj, features, labels, target_node, device=device)\n",
                "\n",
                "# Run Attack (5 perturbations)\n",
                "# This will modify the graph structure to fool the GCN\n",
                "modified_adj = nettack.attack(n_perturbations=5)\n",
                "\n",
                "# Evaluate on perturbed graph\n",
                "# Convert the modified scipy sparse matrix back to PyTorch Geometric edge_index\n",
                "modified_edge_index, _ = from_scipy_sparse_matrix(modified_adj)\n",
                "modified_edge_index = modified_edge_index.to(device)\n",
                "\n",
                "model.eval()\n",
                "out_pert = model(data.x, modified_edge_index)\n",
                "pred_pert = out_pert.argmax(dim=1)\n",
                "probs_pert = torch.exp(out_pert)\n",
                "\n",
                "print(\"\\n--- After Attack ---\")\n",
                "print(f\"Prediction: {pred_pert[target_node].item()} (Conf: {probs_pert[target_node, pred_pert[target_node]]:.4f})\")\n",
                "if pred_pert[target_node] != data.y[target_node]:\n",
                "    print(\"SUCCESS: Attack flipped the label!\")\n",
                "else:\n",
                "    print(\"FAILURE: Label not flipped.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}